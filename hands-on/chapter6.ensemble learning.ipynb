{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)), ('rf', RandomF...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)   # 이거맞냐..?\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr',log_clf),('rf',rnd_clf),('svc',svm_clf)],\n",
    "    voting='hard')\n",
    "\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 1.0\n",
      "RandomForestClassifier 1.0\n",
      "SVC 1.0\n",
      "VotingClassifier 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf,svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련세트에서 중복을 허용하여 샘플링하는 방식: 배깅\n",
    "# 중복을 허용하지 않고 샘플링하는 방식: 페이스팅\n",
    "# 부트스트래핑은 각 예측기가 학습하는 서브셋에 다양성을 증가시키므로 배깅이 페이스팅보다 편향이 조금 더 놓다.\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=50, bootstrap=True, n_jobs=-1)  # max_samples = 100: error, n_jobs = -1: CPU 코어 지정\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94999999999999996"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "bag_clf.fit(X_train,y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.23157895,  0.76842105],\n",
       "       [ 0.81920904,  0.18079096],\n",
       "       [ 0.06508876,  0.93491124],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.921875  ,  0.078125  ],\n",
       "       [ 0.89944134,  0.10055866],\n",
       "       [ 0.94148936,  0.05851064],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.73493976,  0.26506024],\n",
       "       [ 0.03867403,  0.96132597],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.76111111,  0.23888889],\n",
       "       [ 0.01212121,  0.98787879],\n",
       "       [ 0.14761905,  0.85238095],\n",
       "       [ 0.09944751,  0.90055249],\n",
       "       [ 0.90659341,  0.09340659],\n",
       "       [ 0.55670103,  0.44329897],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.00529101,  0.99470899],\n",
       "       [ 0.82122905,  0.17877095],\n",
       "       [ 0.01169591,  0.98830409],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.00526316,  0.99473684],\n",
       "       [ 0.97142857,  0.02857143],\n",
       "       [ 0.99459459,  0.00540541],\n",
       "       [ 0.98809524,  0.01190476],\n",
       "       [ 0.01104972,  0.98895028],\n",
       "       [ 0.88297872,  0.11702128],\n",
       "       [ 0.90960452,  0.09039548],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.01242236,  0.98757764],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.80540541,  0.19459459],\n",
       "       [ 0.99421965,  0.00578035],\n",
       "       [ 0.38547486,  0.61452514],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.21428571,  0.78571429],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.03482587,  0.96517413],\n",
       "       [ 0.4040404 ,  0.5959596 ],\n",
       "       [ 0.99431818,  0.00568182],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.29310345,  0.70689655],\n",
       "       [ 0.09134615,  0.90865385],\n",
       "       [ 0.7625    ,  0.2375    ],\n",
       "       [ 0.98529412,  0.01470588],\n",
       "       [ 0.04975124,  0.95024876],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.97206704,  0.02793296],\n",
       "       [ 0.94475138,  0.05524862],\n",
       "       [ 0.71938776,  0.28061224],\n",
       "       [ 0.02197802,  0.97802198],\n",
       "       [ 0.08092486,  0.91907514],\n",
       "       [ 0.93296089,  0.06703911],\n",
       "       [ 0.00540541,  0.99459459],\n",
       "       [ 0.01570681,  0.98429319],\n",
       "       [ 0.01156069,  0.98843931],\n",
       "       [ 0.47282609,  0.52717391],\n",
       "       [ 0.15819209,  0.84180791],\n",
       "       [ 0.11695906,  0.88304094],\n",
       "       [ 0.99019608,  0.00980392],\n",
       "       [ 0.00534759,  0.99465241],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.00555556,  0.99444444],\n",
       "       [ 0.08947368,  0.91052632],\n",
       "       [ 0.10674157,  0.89325843],\n",
       "       [ 0.17821782,  0.82178218],\n",
       "       [ 0.03553299,  0.96446701],\n",
       "       [ 0.01117318,  0.98882682],\n",
       "       [ 0.00568182,  0.99431818],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.06532663,  0.93467337],\n",
       "       [ 1.        ,  0.        ],\n",
       "       [ 0.04591837,  0.95408163],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.97590361,  0.02409639]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes = 16, n_jobs=-1)  # 500개 트리로 이뤄진 랜덤포레스트 분류기\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),   # 무작위 분학 -> 최상의 분할을 선택\n",
    "    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)\n",
    "\n",
    "# 극단적으로 무작위한 트리의 랜덤포레스트를 익스트림 랜덤 트리 앙상블이라고 부른다.\n",
    "# 편향이 늘어나지만 분산을 낮춘다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.0974849782894\n",
      "sepal width (cm) 0.0237384844024\n",
      "petal length (cm) 0.431884728475\n",
      "petal width (cm) 0.446891808833\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "\n",
    "for name, score in zip(iris[\"feature_names\"],rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 픽셀 중요도\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# # conf_mx = confusion_matrix(iris[\"feature_names\"],rnd_clf.feature_importances_)\n",
    "\n",
    "# plt.matshow(rnd_clf.feature_importances_)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "          learning_rate=0.5, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 부스팅은 약한 학습기를 여러개 연결하여 강한 학습기를 만드는 앙상블 학습방법.\n",
    "# 아다부스트:이전 모델이 과속적합했던 훈련 샘플의 가중치를 넢여 새로운 예측기를 학습하기 어려운 샘플에 점점 더 맞추는 방법\n",
    "# SAMME: 아다부스트의 다중 클래스 버전\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,   # max_depth=1: 결정노드 하나와 리프 노드 두개로 이루어진 결정트리\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5)\n",
    "\n",
    "ada_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 그래디언트 부스팅은 앙상블에서 이전까지의 잔여 오차를 보정하여 새로운 예측기를 학습시키는 방법\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X,y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = sum(tree.predict(X_new) for tree in (tree_reg1,tree_reg2,tree_reg3))\n",
    "y_pred = sum(tree.predict(X) for tree in (tree_reg1,tree_reg2,tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=1.0, loss='ls', max_depth=2, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=3, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=2, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=14, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GBRT앙상블 훈련\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    "          for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 조기종료 구현하기\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True) # subsample=0.25 : SGB(Stochastic Gradient Boosting)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1,120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break  # 조기종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
